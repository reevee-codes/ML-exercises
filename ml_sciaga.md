# ğŸ§  Podstawowe pojÄ™cia ML -- uporzÄ…dkowana Å›ciÄ…ga

## Regresja liniowa

Najprostszy model ML: y = w1*x1 + w2*x2 + w3\*x3 + b. Model uczy siÄ™ wag
minimalizujÄ…c bÅ‚Ä…d.

## Wagi

OkreÅ›lajÄ… wpÅ‚yw cech na wynik. Uczenie = dopasowanie wag.

## Funkcja bÅ‚Ä™du

error = target - prediction. Model minimalizuje bÅ‚Ä…d.

## Gradient Descent

Aktualizacja wag: w = w + error \* input \* learning_rate

## Learning Rate

WielkoÅ›Ä‡ kroku uczenia. 0.1 -- za duÅ¼y, 0.001 -- bezpieczny.

## Epoka

Jedno przejÅ›cie przez dane treningowe.

## Convergence

Moment gdy bÅ‚Ä…d przestaje maleÄ‡ â†’ zatrzymujemy trening.

## Normalizacja danych

Skalowanie danych do zakresu 0--1 stabilizuje uczenie.

## Exploding Gradients

Zbyt duÅ¼e wagi â†’ overflow â†’ NaN.

## Floating Point Overflow

Limit float â‰ˆ 1.8e308 â†’ po przekroczeniu pojawia siÄ™ inf i NaN.

## Overtraining

Za duÅ¼o epok â†’ brak poprawy i ryzyko niestabilnoÅ›ci.

## Early Stopping

Automatyczne zatrzymanie gdy bÅ‚Ä…d przestaje maleÄ‡.

## Pipeline ML

1.  Przygotuj dane
2.  Znormalizuj dane
3.  Trenuj model
4.  Monitoruj bÅ‚Ä…d
5.  Zatrzymaj trening
6.  UÅ¼yj modelu

## NajczÄ™stsze powody NaN

-   za duÅ¼y learning rate
-   brak normalizacji
-   exploding gradients
-   zbyt dÅ‚ugi trening
